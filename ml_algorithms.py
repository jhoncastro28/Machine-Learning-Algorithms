import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.svm import SVC, SVR
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve, mean_squared_error, mean_absolute_error, r2_score, balanced_accuracy_score, f1_score, average_precision_score
import warnings
import os
import sys
import json
import shutil
import subprocess
from pathlib import Path
import kagglehub
import argparse
from typing import Optional, Tuple

warnings.filterwarnings('ignore')
plt.style.use('default')
sns.set_palette("husl")

class CoffeeShopMLAnalysis:
    def __init__(self):
        self.data = None
        self.X = None
        self.y = None
        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_test = None
        self.scaler = StandardScaler()
        self.models = {}
        self.results = {}
        self.resolved_data_path = None
        self.resolved_source = None
        
    def _validate_kaggle_credentials(self) -> Tuple[bool, str]:
        """Valida credenciales y binario de Kaggle API en Windows/Linux/Mac.

        Retorna (ok, mensaje_error_si_falla).
        """
        kaggle_json_path = os.path.expanduser("~/.kaggle/kaggle.json")
        if not os.path.isfile(kaggle_json_path):
            return False, (
                "No se encontr√≥ ~/.kaggle/kaggle.json. Descarga tu token desde Kaggle (Account > API) y col√≥calo en esa ruta."
            )
        try:
            with open(kaggle_json_path, "r", encoding="utf-8") as f:
                cfg = json.load(f)
            if not cfg.get("username") or not cfg.get("key"):
                return False, "kaggle.json inv√°lido: faltan 'username' o 'key'."
        except Exception as e:
            return False, f"No se pudo leer kaggle.json: {e}"

        kaggle_bin = shutil.which("kaggle")
        if kaggle_bin is None:
            return False, (
                "CLI de Kaggle no encontrada. Instala con 'pip install kaggle' y a√±ade al PATH."
            )
        return True, ""

    def _download_from_kaggle(self, dataset: str, dest_dir: str) -> str:
        """Descarga dataset desde Kaggle usando CLI oficial. Retorna ruta esperada del CSV si se puede inferir.

        Lanza excepci√≥n con mensaje √∫til si falla.
        """
        ok, err = self._validate_kaggle_credentials()
        if not ok:
            raise RuntimeError(err)

        Path(dest_dir).mkdir(parents=True, exist_ok=True)
        cmd = [
            "kaggle", "datasets", "download", "-d", dataset, "-p", dest_dir, "--unzip"
        ]
        try:
            subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        except subprocess.CalledProcessError as e:
            raise RuntimeError(
                f"Fallo descarga Kaggle: {e.stderr.decode(errors='ignore') or e.stdout.decode(errors='ignore')}"
            )

        # Heur√≠stica: si hay un √∫nico CSV en dest_dir tras unzip, lo usamos.
        csv_files = list(Path(dest_dir).glob("*.csv"))
        if len(csv_files) == 1:
            return str(csv_files[0])
        return ""

    def resolve_and_load_data(self,
                              path: Optional[str] = None,
                              source: str = "kaggle",
                              kaggle_dataset: str = "himelsarder/coffee-shop-daily-revenue-prediction-dataset",
                              filename_hint: str = "coffee_shop_revenue.csv",
                              sep: str = ",",
                              decimal: str = ".",
                              id_column: Optional[str] = None,
                              demo: bool = False) -> None:
        """Resuelve y carga datos con fallback claro:
        1) CSV expl√≠cito (--path)
        2) Carpeta local data/ (filename_hint)
        3) Kaggle (dataset por defecto u opcional)
        4) Error √∫til (salvo --demo)
        """
        # 1) CSV expl√≠cito
        if path:
            abs_path = os.path.abspath(path)
            if not os.path.isfile(abs_path):
                raise FileNotFoundError(f"No existe el archivo especificado: {abs_path}")
            self.data = pd.read_csv(abs_path, sep=sep, decimal=decimal)
            if id_column and id_column in self.data.columns:
                self.data = self.data.drop(columns=[id_column])
            self.resolved_data_path = abs_path
            self.resolved_source = "local:file"
            print(f"Usando archivo proporcionado: {self.resolved_data_path}")
            return

        # 2) Carpeta data/
        local_dir = os.path.abspath("data")
        local_candidate = os.path.join(local_dir, filename_hint)
        if os.path.isfile(local_candidate):
            self.data = pd.read_csv(local_candidate, sep=sep, decimal=decimal)
            if id_column and id_column in self.data.columns:
                self.data = self.data.drop(columns=[id_column])
            self.resolved_data_path = local_candidate
            self.resolved_source = "local:data_dir"
            print(f"Usando archivo local en data/: {self.resolved_data_path}")
            return

        # 3) Kaggle (si source permite)
        if source.lower() == "kaggle":
            try:
                print("Descargando dataset desde Kaggle (CLI oficial)...")
                inferred_csv = self._download_from_kaggle(kaggle_dataset, local_dir)
                # Preferir filename_hint si existe; si no, usar inferido
                final_csv = (
                    os.path.join(local_dir, filename_hint)
                    if os.path.isfile(os.path.join(local_dir, filename_hint))
                    else inferred_csv
                )
                if not final_csv or not os.path.isfile(final_csv):
                    raise FileNotFoundError(
                        "Descarga realizada, pero no se encontr√≥ el CSV esperado. Revisa el contenido de data/."
                    )
                self.data = pd.read_csv(final_csv, sep=sep, decimal=decimal)
                if id_column and id_column in self.data.columns:
                    self.data = self.data.drop(columns=[id_column])
                self.resolved_data_path = os.path.abspath(final_csv)
                self.resolved_source = "kaggle"
                print(f"Usando archivo descargado de Kaggle: {self.resolved_data_path}")
                return
            except Exception as e:
                print(f"Error Kaggle: {e}")
                print("Intentando fallback opcional con kagglehub...")
                try:
                    path_dir = kagglehub.dataset_download(kaggle_dataset)
                    candidate = os.path.join(path_dir, filename_hint)
                    if not os.path.isfile(candidate):
                        # Si no coincide el nombre, intentar primer CSV
                        csvs = list(Path(path_dir).glob("*.csv"))
                        if not csvs:
                            raise FileNotFoundError("kagglehub no entreg√≥ CSVs.")
                        candidate = str(csvs[0])
                    self.data = pd.read_csv(candidate, sep=sep, decimal=decimal)
                    if id_column and id_column in self.data.columns:
                        self.data = self.data.drop(columns=[id_column])
                    self.resolved_data_path = os.path.abspath(candidate)
                    self.resolved_source = "kagglehub"
                    print(f"Usando archivo desde kagglehub: {self.resolved_data_path}")
                    return
                except Exception as e2:
                    print(f"Error kagglehub: {e2}")

        # 4) Error claro (o demo)
        if demo:
            print("No se encontr√≥ dataset. Modo --demo activo: generando datos sint√©ticos...")
            self.create_synthetic_data()
            self.resolved_data_path = "<synthetic>"
            self.resolved_source = "demo"
            return
        raise RuntimeError(
            "No se pudo resolver un archivo de datos. Proporcione --path v√°lido, coloque el CSV en data/, o habilite Kaggle."
        )

    def run_via_target(self, target: str = "classification"):
        """Ejecuta flujo seg√∫n target: regression|classification|both."""
        self.exploratory_data_analysis()
        if target == "regression":
            self.evaluate_regression_with_pipelines(cv_splits=5)
            return
        if target == "classification":
            # Mantener el flujo cl√°sico de clasificaci√≥n existente
            self.prepare_data()
            self.initialize_models()
            self.train_and_evaluate_models()
            self.create_comparison_visualizations()
            self.create_results_table()
            self.generate_detailed_report()
            best_model = max(self.results.keys(), key=lambda k: self.results[k]['accuracy'])
            if best_model in ['Random Forest', 'SVM']:
                self.hyperparameter_tuning(best_model)
            return
        # both
        self.run_objective_evaluations()


def build_arg_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(description="Coffee Shop ML - Flexible data input")
    parser.add_argument("--path", type=str, default=None, help="Ruta a CSV local")
    parser.add_argument("--source", type=str, default="kaggle", choices=["kaggle", "local"], help="Fuente de datos")
    parser.add_argument("--target", type=str, default="classification", choices=["classification", "regression", "both"], help="Tarea objetivo")
    parser.add_argument("--sep", type=str, default=",", help="Separador de columnas del CSV")
    parser.add_argument("--decimal", type=str, default=".", help="Separador decimal del CSV")
    parser.add_argument("--id-column", dest="id_column", type=str, default=None, help="Nombre de columna ID a descartar")
    parser.add_argument("--kaggle-dataset", dest="kaggle_dataset", type=str, default="himelsarder/coffee-shop-daily-revenue-prediction-dataset", help="Slug de dataset Kaggle datasets")
    parser.add_argument("--filename-hint", dest="filename_hint", type=str, default="coffee_shop_revenue.csv", help="Nombre de archivo esperado dentro del dataset")
    parser.add_argument("--demo", action="store_true", help="Permite generar datos sint√©ticos si no hay fuentes disponibles")
    return parser

    def download_and_load_data(self):
        """Descarga y carga el dataset desde Kaggle"""
        try:
            print("Descargando dataset desde Kaggle...")
            path = kagglehub.dataset_download("himelsarder/coffee-shop-daily-revenue-prediction-dataset")
            print(f"Dataset descargado en: {path}")
            
            # Cargar el dataset
            self.data = pd.read_csv(f"{path}/coffee_shop_revenue.csv")
            print("Dataset cargado exitosamente!")
            return True
        except Exception as e:
            print(f"Error al descargar desde Kaggle: {e}")
            print("Intentando cargar archivo local...")
            try:
                self.data = pd.read_csv("coffee_shop_revenue.csv")
                print("Dataset cargado desde archivo local!")
                return True
            except:
                print("No se pudo cargar el dataset. Creando datos sint√©ticos para demostraci√≥n...")
                self.create_synthetic_data()
                return True
    
    def create_synthetic_data(self):
        """Crea datos sint√©ticos para demostraci√≥n si no se puede descargar el dataset real"""
        np.random.seed(42)
        n_samples = 1000
        
        data = {
            'day_of_week': np.random.choice(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'], n_samples),
            'weather': np.random.choice(['Sunny', 'Rainy', 'Cloudy'], n_samples),
            'temperature': np.random.normal(20, 8, n_samples),
            'customers': np.random.poisson(50, n_samples),
            'promotion': np.random.choice([0, 1], n_samples),
            'holiday': np.random.choice([0, 1], n_samples, p=[0.9, 0.1]),
            'season': np.random.choice(['Spring', 'Summer', 'Fall', 'Winter'], n_samples),
        }
        
        # Crear variable objetivo basada en las caracter√≠sticas
        revenue_base = (data['customers'] * 8 + 
                       (data['temperature'] > 15) * 200 + 
                       data['promotion'] * 300 + 
                       data['holiday'] * 400 +
                       np.random.normal(0, 100, n_samples))
        
        data['daily_revenue'] = np.maximum(revenue_base, 0)
        
        # Crear categor√≠as de ingresos para clasificaci√≥n
        revenue_percentiles = np.percentile(data['daily_revenue'], [33, 66])
        data['revenue_category'] = pd.cut(data['daily_revenue'], 
                                        bins=[0, revenue_percentiles[0], revenue_percentiles[1], np.inf],
                                        labels=['Low', 'Medium', 'High'])
        
        self.data = pd.DataFrame(data)
        print("Datos sint√©ticos creados exitosamente!")
    
    # =====================
    # NUEVO: Preprocesamiento y evaluaciones sin fuga
    # =====================
    def _build_feature_preprocessor(self, X: pd.DataFrame) -> ColumnTransformer:
        """Crea un ColumnTransformer (one-hot + estandarizaci√≥n) para evitar fuga.

        - OneHotEncoder para columnas categ√≥ricas.
        - StandardScaler para columnas num√©ricas.
        - Ajuste ocurre dentro del Pipeline en cada fold.
        """
        categorical_columns = X.select_dtypes(include=["object", "category"]).columns.tolist()
        numeric_columns = X.select_dtypes(include=[np.number]).columns.tolist()

        preprocessor = ColumnTransformer(
            transformers=[
                ("categorical", OneHotEncoder(handle_unknown="ignore"), categorical_columns),
                ("numeric", StandardScaler(), numeric_columns),
            ],
            remainder="drop",
        )
        return preprocessor

    def describe_objective_and_metrics(self):
        """Objetivos y m√©tricas:

        - Regresi√≥n: predecir `daily_revenue`. M√©tricas: RMSE, MAE, R2.
        - Clasificaci√≥n: derivar `revenue_category` por cuantiles (3-4). M√©tricas:
          balanced accuracy, macro F1, ROC-AUC OvR, PR-AUC macro.

        Validaci√≥n sin fuga:
        - Cortes de cuantiles calculados SOLO con el conjunto de entrenamiento (por fold).
        - One-hot/estandarizaci√≥n dentro de un Pipeline evaluado por fold en CV.
        """
        print("\nObjetivo y m√©tricas definidos:")
        print("- Regresi√≥n: RMSE, MAE, R2 sobre `daily_revenue`.")
        print("- Clasificaci√≥n por cuantiles: balanced accuracy, macro F1, ROC-AUC OvR, PR-AUC.")

    def evaluate_regression_with_pipelines(self, cv_splits: int = 5) -> pd.DataFrame:
        """Eval√∫a regresi√≥n con Pipelines (sin fuga) y CV.

        Retorna DataFrame con medias y desviaciones (RMSE, MAE, R2).
        """
        if 'daily_revenue' not in self.data.columns:
            raise ValueError("No se encontr√≥ la columna 'daily_revenue' para regresi√≥n.")

        feature_cols = [c for c in self.data.columns if c != 'daily_revenue']
        X = self.data[feature_cols].copy()
        y = self.data['daily_revenue'].astype(float).values

        preprocessor = self._build_feature_preprocessor(X)

        models = {
            'LinearRegression': LinearRegression(),
            'RandomForestRegressor': RandomForestRegressor(random_state=42, n_estimators=300),
            'SVR(rbf)': SVR(kernel='rbf', C=1.0, gamma='scale'),
        }

        kf = KFold(n_splits=cv_splits, shuffle=True, random_state=42)
        rows = []

        for name, model in models.items():
            pipeline = Pipeline([
                ('preprocess', preprocessor),
                ('model', model),
            ])

            rmse_list, mae_list, r2_list = [], [], []
            for train_idx, val_idx in kf.split(X):
                X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
                y_train, y_val = y[train_idx], y[val_idx]

                pipeline.fit(X_train, y_train)
                y_pred = pipeline.predict(X_val)
                rmse_list.append(np.sqrt(mean_squared_error(y_val, y_pred)))
                mae_list.append(mean_absolute_error(y_val, y_pred))
                r2_list.append(r2_score(y_val, y_pred))

            rows.append({
                'Modelo': name,
                'RMSE_Œº': float(np.mean(rmse_list)),
                'RMSE_œÉ': float(np.std(rmse_list)),
                'MAE_Œº': float(np.mean(mae_list)),
                'MAE_œÉ': float(np.std(mae_list)),
                'R2_Œº': float(np.mean(r2_list)),
                'R2_œÉ': float(np.std(r2_list)),
            })

        results_df = pd.DataFrame(rows)
        print("\nResultados Regresi√≥n (CV):")
        print(results_df.to_string(index=False, float_format=lambda v: f"{v:.4f}"))
        return results_df

    def evaluate_classification_by_quantiles(self, n_bins: int = 3, cv_splits: int = 5) -> pd.DataFrame:
        """Eval√∫a clasificaci√≥n derivando categor√≠as por cuantiles de `daily_revenue`.

        Reglas anti-fuga: cortes con y_train por fold y preprocesamiento en Pipeline.
        """
        if 'daily_revenue' not in self.data.columns:
            raise ValueError("No se encontr√≥ 'daily_revenue' para cuantiles.")

        feature_cols = [c for c in self.data.columns if c != 'daily_revenue']
        X = self.data[feature_cols].copy()
        y_cont = self.data['daily_revenue'].astype(float).values

        preprocessor = self._build_feature_preprocessor(X)

        models = {
            'LogisticRegression': LogisticRegression(max_iter=1000, random_state=42),
            'RandomForestClassifier': RandomForestClassifier(n_estimators=300, random_state=42),
            'SVC(prob)': SVC(probability=True, kernel='rbf', gamma='scale', C=1.0, random_state=42),
        }

        kf = KFold(n_splits=cv_splits, shuffle=True, random_state=42)
        rows = []

        for name, model in models.items():
            pipeline = Pipeline([
                ('preprocess', preprocessor),
                ('model', model),
            ])

            bal_acc_list, f1_macro_list, roc_auc_list, pr_auc_list = [], [], [], []

            for train_idx, val_idx in kf.split(X):
                X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
                y_train_cont, y_val_cont = y_cont[train_idx], y_cont[val_idx]

                # Cortes por cuantiles SOLO con train
                q = np.linspace(0, 1, n_bins + 1)
                cut_points = np.quantile(y_train_cont, q)
                cut_points = np.unique(cut_points)
                if len(cut_points) < 2:
                    continue

                def bin_with_cutpoints(values: np.ndarray) -> np.ndarray:
                    labels = list(range(len(cut_points) - 1))
                    return pd.cut(values, bins=cut_points, labels=labels, include_lowest=True).astype(int).values

                y_train_cls = bin_with_cutpoints(y_train_cont)
                y_val_cls = bin_with_cutpoints(y_val_cont)

                pipeline.fit(X_train, y_train_cls)
                y_pred = pipeline.predict(X_val)

                # Probabilidades para AUC/PR-AUC
                if hasattr(pipeline.named_steps['model'], 'predict_proba'):
                    y_proba = pipeline.predict_proba(X_val)
                else:
                    if hasattr(pipeline.named_steps['model'], 'decision_function'):
                        scores = pipeline.decision_function(X_val)
                        exp_scores = np.exp(scores - np.max(scores, axis=1, keepdims=True))
                        y_proba = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)
                    else:
                        preds = y_pred
                        num_classes = len(np.unique(y_train_cls))
                        y_proba = np.eye(num_classes)[preds]

                bal_acc_list.append(balanced_accuracy_score(y_val_cls, y_pred))
                f1_macro_list.append(f1_score(y_val_cls, y_pred, average='macro'))
                try:
                    roc_auc_list.append(roc_auc_score(y_val_cls, y_proba, multi_class='ovr'))
                except Exception:
                    pass
                try:
                    pr_auc_list.append(average_precision_score(pd.get_dummies(y_val_cls).values, y_proba, average='macro'))
                except Exception:
                    pass

            rows.append({
                'Modelo': name,
                'BalancedAcc_Œº': float(np.mean(bal_acc_list)) if bal_acc_list else np.nan,
                'BalancedAcc_œÉ': float(np.std(bal_acc_list)) if bal_acc_list else np.nan,
                'F1_macro_Œº': float(np.mean(f1_macro_list)) if f1_macro_list else np.nan,
                'F1_macro_œÉ': float(np.std(f1_macro_list)) if f1_macro_list else np.nan,
                'ROC-AUC_OvR_Œº': float(np.mean(roc_auc_list)) if roc_auc_list else np.nan,
                'ROC-AUC_OvR_œÉ': float(np.std(roc_auc_list)) if roc_auc_list else np.nan,
                'PR-AUC_macro_Œº': float(np.mean(pr_auc_list)) if pr_auc_list else np.nan,
                'PR-AUC_macro_œÉ': float(np.std(pr_auc_list)) if pr_auc_list else np.nan,
            })

        results_df = pd.DataFrame(rows)
        print("\nResultados Clasificaci√≥n por cuantiles (CV):")
        print(results_df.to_string(index=False, float_format=lambda v: f"{v:.4f}"))
        return results_df

    def run_objective_evaluations(self):
        """Ejecuta ambas tareas (regresi√≥n y clasificaci√≥n por cuantiles) con CV sin fuga."""
        self.describe_objective_and_metrics()
        reg = self.evaluate_regression_with_pipelines(cv_splits=5)
        cls = self.evaluate_classification_by_quantiles(n_bins=3, cv_splits=5)
        return reg, cls
    def exploratory_data_analysis(self):
        """Realiza an√°lisis exploratorio de datos"""
        print("\n" + "="*50)
        print("AN√ÅLISIS EXPLORATORIO DE DATOS")
        print("="*50)
        
        print("\n1. Informaci√≥n b√°sica del dataset:")
        print(f"Forma del dataset: {self.data.shape}")
        print(f"Columnas: {list(self.data.columns)}")
        
        print("\n2. Tipos de datos:")
        print(self.data.dtypes)
        
        print("\n3. Valores nulos:")
        print(self.data.isnull().sum())
        
        print("\n4. Estad√≠sticas descriptivas:")
        print(self.data.describe())
        
        if 'revenue_category' in self.data.columns:
            print("\n5. Distribuci√≥n de la variable objetivo:")
            print(self.data['revenue_category'].value_counts())
        
        # Crear visualizaciones
        self.create_eda_visualizations()
    
    def create_eda_visualizations(self):
        """Crea visualizaciones para el an√°lisis exploratorio"""
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('An√°lisis Exploratorio de Datos - Cafeter√≠a', fontsize=16, fontweight='bold')
        
        # Distribuci√≥n de ingresos
        if 'daily_revenue' in self.data.columns:
            axes[0, 0].hist(self.data['daily_revenue'], bins=30, color='skyblue', alpha=0.7, edgecolor='black')
            axes[0, 0].set_title('Distribuci√≥n de Ingresos Diarios')
            axes[0, 0].set_xlabel('Ingresos ($)')
            axes[0, 0].set_ylabel('Frecuencia')
        
        # Distribuci√≥n de categor√≠as de ingresos
        if 'revenue_category' in self.data.columns:
            revenue_counts = self.data['revenue_category'].value_counts()
            axes[0, 1].bar(revenue_counts.index, revenue_counts.values, color=['lightcoral', 'lightgreen', 'lightblue'])
            axes[0, 1].set_title('Distribuci√≥n por Categor√≠a de Ingresos')
            axes[0, 1].set_ylabel('Frecuencia')
        
        # Ingresos por d√≠a de la semana
        if 'day_of_week' in self.data.columns and 'daily_revenue' in self.data.columns:
            day_revenue = self.data.groupby('day_of_week')['daily_revenue'].mean()
            axes[0, 2].bar(day_revenue.index, day_revenue.values, color='orange', alpha=0.7)
            axes[0, 2].set_title('Ingresos Promedio por D√≠a de la Semana')
            axes[0, 2].set_ylabel('Ingresos Promedio ($)')
            axes[0, 2].tick_params(axis='x', rotation=45)
        
        # Correlaciones (solo variables num√©ricas)
        numeric_cols = self.data.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 1:
            correlation = self.data[numeric_cols].corr()
            sns.heatmap(correlation, annot=True, cmap='coolwarm', center=0, ax=axes[1, 0])
            axes[1, 0].set_title('Matriz de Correlaci√≥n')
        
        # Clientes vs Ingresos
        if 'customers' in self.data.columns and 'daily_revenue' in self.data.columns:
            axes[1, 1].scatter(self.data['customers'], self.data['daily_revenue'], alpha=0.6, color='purple')
            axes[1, 1].set_title('Clientes vs Ingresos')
            axes[1, 1].set_xlabel('N√∫mero de Clientes')
            axes[1, 1].set_ylabel('Ingresos ($)')
        
        # Temperatura vs Ingresos
        if 'temperature' in self.data.columns and 'daily_revenue' in self.data.columns:
            axes[1, 2].scatter(self.data['temperature'], self.data['daily_revenue'], alpha=0.6, color='red')
            axes[1, 2].set_title('Temperatura vs Ingresos')
            axes[1, 2].set_xlabel('Temperatura (¬∞C)')
            axes[1, 2].set_ylabel('Ingresos ($)')
        
        plt.tight_layout()
        plt.show()
    
    def prepare_data(self):
        """Prepara los datos para machine learning"""
        print("\n" + "="*50)
        print("PREPARACI√ìN DE DATOS")
        print("="*50)
        
        # Si no existe revenue_category, crearla
        if 'revenue_category' not in self.data.columns and 'daily_revenue' in self.data.columns:
            revenue_percentiles = np.percentile(self.data['daily_revenue'], [33, 66])
            self.data['revenue_category'] = pd.cut(self.data['daily_revenue'], 
                                                 bins=[0, revenue_percentiles[0], revenue_percentiles[1], np.inf],
                                                 labels=['Low', 'Medium', 'High'])
        
        # Separar caracter√≠sticas y variable objetivo
        target_col = 'revenue_category'
        feature_cols = [col for col in self.data.columns if col not in [target_col, 'daily_revenue']]
        
        X = self.data[feature_cols].copy()
        y = self.data[target_col].copy()
        
        # Codificar variables categ√≥ricas
        categorical_columns = X.select_dtypes(include=['object']).columns
        label_encoders = {}
        
        for col in categorical_columns:
            le = LabelEncoder()
            X[col] = le.fit_transform(X[col].astype(str))
            label_encoders[col] = le
        
        # Codificar variable objetivo
        le_target = LabelEncoder()
        y = le_target.fit_transform(y)
        
        print(f"Caracter√≠sticas utilizadas: {feature_cols}")
        print(f"Forma de X: {X.shape}")
        print(f"Distribuci√≥n de y: {np.bincount(y)}")
        
        # Dividir en entrenamiento y prueba
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # Normalizar caracter√≠sticas
        self.X_train_scaled = self.scaler.fit_transform(self.X_train)
        self.X_test_scaled = self.scaler.transform(self.X_test)
        
        self.X = X
        self.y = y
        
        print(f"Datos de entrenamiento: {self.X_train.shape}")
        print(f"Datos de prueba: {self.X_test.shape}")
    
    def initialize_models(self):
        """Inicializa los modelos de machine learning"""
        self.models = {
            'Regresi√≥n Log√≠stica': LogisticRegression(random_state=42, max_iter=1000),
            'SVM': SVC(random_state=42, probability=True),
            '√Årbol de Decisi√≥n': DecisionTreeClassifier(random_state=42),
            'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),
            'Red Neuronal': MLPClassifier(random_state=42, max_iter=1000, hidden_layer_sizes=(100, 50))
        }
    
    def train_and_evaluate_models(self):
        """Entrena y eval√∫a todos los modelos"""
        print("\n" + "="*50)
        print("ENTRENAMIENTO Y EVALUACI√ìN DE MODELOS")
        print("="*50)
        
        self.results = {}
        
        for name, model in self.models.items():
            print(f"\nEntrenando {name}...")
            
            # Usar datos escalados para modelos que lo requieren
            if name in ['Regresi√≥n Log√≠stica', 'SVM', 'Red Neuronal']:
                X_train_use = self.X_train_scaled
                X_test_use = self.X_test_scaled
            else:
                X_train_use = self.X_train
                X_test_use = self.X_test
            
            # Entrenar modelo
            model.fit(X_train_use, self.y_train)
            
            # Predicciones
            y_pred = model.predict(X_test_use)
            y_pred_proba = model.predict_proba(X_test_use)
            
            # M√©tricas
            accuracy = accuracy_score(self.y_test, y_pred)
            
            # Validaci√≥n cruzada
            cv_scores = cross_val_score(model, X_train_use, self.y_train, cv=5)
            
            # AUC para multiclase (promedio)
            try:
                auc_score = roc_auc_score(self.y_test, y_pred_proba, multi_class='ovr')
            except:
                auc_score = 0
            
            # Guardar resultados
            self.results[name] = {
                'model': model,
                'accuracy': accuracy,
                'cv_mean': cv_scores.mean(),
                'cv_std': cv_scores.std(),
                'auc': auc_score,
                'y_pred': y_pred,
                'y_pred_proba': y_pred_proba,
                'classification_report': classification_report(self.y_test, y_pred, output_dict=True)
            }
            
            print(f"Precisi√≥n: {accuracy:.4f}")
            print(f"CV Score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")
            print(f"AUC: {auc_score:.4f}")
    
    def create_comparison_visualizations(self):
        """Crea visualizaciones comparativas de los modelos"""
        fig, axes = plt.subplots(2, 3, figsize=(20, 12))
        fig.suptitle('Comparaci√≥n de Algoritmos de Machine Learning', fontsize=16, fontweight='bold')
        
        # 1. Comparaci√≥n de precisi√≥n
        models = list(self.results.keys())
        accuracies = [self.results[model]['accuracy'] for model in models]
        
        axes[0, 0].bar(models, accuracies, color='lightblue', alpha=0.7, edgecolor='black')
        axes[0, 0].set_title('Precisi√≥n por Modelo')
        axes[0, 0].set_ylabel('Precisi√≥n')
        axes[0, 0].tick_params(axis='x', rotation=45)
        axes[0, 0].set_ylim(0, 1)
        
        # 2. Comparaci√≥n CV Score
        cv_means = [self.results[model]['cv_mean'] for model in models]
        cv_stds = [self.results[model]['cv_std'] for model in models]
        
        axes[0, 1].bar(models, cv_means, yerr=cv_stds, color='lightgreen', alpha=0.7, 
                      capsize=5, edgecolor='black')
        axes[0, 1].set_title('Validaci√≥n Cruzada (5-fold)')
        axes[0, 1].set_ylabel('CV Score')
        axes[0, 1].tick_params(axis='x', rotation=45)
        axes[0, 1].set_ylim(0, 1)
        
        # 3. Comparaci√≥n AUC
        auc_scores = [self.results[model]['auc'] for model in models]
        axes[0, 2].bar(models, auc_scores, color='lightcoral', alpha=0.7, edgecolor='black')
        axes[0, 2].set_title('AUC Score')
        axes[0, 2].set_ylabel('AUC')
        axes[0, 2].tick_params(axis='x', rotation=45)
        axes[0, 2].set_ylim(0, 1)
        
        # 4. Matriz de confusi√≥n del mejor modelo
        best_model = max(self.results.keys(), key=lambda k: self.results[k]['accuracy'])
        cm = confusion_matrix(self.y_test, self.results[best_model]['y_pred'])
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1, 0])
        axes[1, 0].set_title(f'Matriz de Confusi√≥n - {best_model}')
        axes[1, 0].set_xlabel('Predicci√≥n')
        axes[1, 0].set_ylabel('Actual')
        
        # 5. Comparaci√≥n de m√©tricas por clase (F1-score)
        f1_scores = {}
        for model in models:
            report = self.results[model]['classification_report']
            f1_scores[model] = report['macro avg']['f1-score']
        
        axes[1, 1].bar(f1_scores.keys(), f1_scores.values(), color='orange', alpha=0.7, edgecolor='black')
        axes[1, 1].set_title('F1-Score Macro Average')
        axes[1, 1].set_ylabel('F1-Score')
        axes[1, 1].tick_params(axis='x', rotation=45)
        axes[1, 1].set_ylim(0, 1)
        
        # 6. Tiempo de entrenamiento (simulado para demostraci√≥n)
        training_times = {
            'Regresi√≥n Log√≠stica': 0.05,
            'SVM': 0.8,
            '√Årbol de Decisi√≥n': 0.1,
            'Random Forest': 0.3,
            'Red Neuronal': 2.0
        }
        
        axes[1, 2].bar(training_times.keys(), training_times.values(), 
                      color='purple', alpha=0.7, edgecolor='black')
        axes[1, 2].set_title('Tiempo de Entrenamiento (relativo)')
        axes[1, 2].set_ylabel('Tiempo (segundos)')
        axes[1, 2].tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        plt.show()
    
    def create_results_table(self):
        """Crea tabla comparativa de resultados"""
        print("\n" + "="*80)
        print("TABLA COMPARATIVA DE RESULTADOS")
        print("="*80)
        
        # Crear DataFrame con resultados
        results_data = []
        for model_name, result in self.results.items():
            results_data.append({
                'Algoritmo': model_name,
                'Precisi√≥n': f"{result['accuracy']:.4f}",
                'CV Score (Œº)': f"{result['cv_mean']:.4f}",
                'CV Score (œÉ)': f"{result['cv_std']:.4f}",
                'AUC': f"{result['auc']:.4f}",
                'F1-Score': f"{result['classification_report']['macro avg']['f1-score']:.4f}",
                'Recall': f"{result['classification_report']['macro avg']['recall']:.4f}",
                'Precision': f"{result['classification_report']['macro avg']['precision']:.4f}"
            })
        
        results_df = pd.DataFrame(results_data)
        print(results_df.to_string(index=False))
        
        # Encontrar el mejor modelo
        best_model = max(self.results.keys(), key=lambda k: self.results[k]['accuracy'])
        print(f"\nüèÜ MEJOR MODELO: {best_model}")
        print(f"   Precisi√≥n: {self.results[best_model]['accuracy']:.4f}")
        print(f"   CV Score: {self.results[best_model]['cv_mean']:.4f}")
        
        return results_df
    
    def generate_detailed_report(self):
        """Genera reporte detallado para el informe"""
        print("\n" + "="*80)
        print("REPORTE DETALLADO - AN√ÅLISIS DE ALGORITMOS ML")
        print("="*80)
        
        print("\n1. DESCRIPCI√ìN DEL PROBLEMA:")
        print("   - Clasificaci√≥n de ingresos diarios de cafeter√≠as")
        print("   - 3 categor√≠as: Bajo, Medio, Alto")
        print("   - Variables: d√≠a de la semana, clima, temperatura, clientes, etc.")
        
        print("\n2. ALGORITMOS EVALUADOS:")
        algorithms = {
            'Regresi√≥n Log√≠stica': 'Modelo lineal para clasificaci√≥n, bueno como baseline',
            'SVM': 'M√°quinas de Vector Soporte, efectivo en espacios de alta dimensi√≥n',
            '√Årbol de Decisi√≥n': 'Modelo interpretable basado en reglas de decisi√≥n',
            'Random Forest': 'Ensemble de √°rboles, reduce overfitting',
            'Red Neuronal': 'Modelo no lineal, captura relaciones complejas'
        }
        
        for algo, description in algorithms.items():
            print(f"   - {algo}: {description}")
        
        print("\n3. PROTOCOLO DE EVALUACI√ìN:")
        print("   - Divisi√≥n 80/20 para entrenamiento/prueba")
        print("   - Validaci√≥n cruzada 5-fold")
        print("   - M√©tricas: Precisi√≥n, F1-Score, AUC, Recall, Precision")
        print("   - Normalizaci√≥n para modelos sensibles a escala")
        
        print("\n4. RESULTADOS PRINCIPALES:")
        for model_name, result in self.results.items():
            print(f"   {model_name}:")
            print(f"     - Precisi√≥n: {result['accuracy']:.3f}")
            print(f"     - CV Score: {result['cv_mean']:.3f} ¬± {result['cv_std']:.3f}")
            print(f"     - F1-Score: {result['classification_report']['macro avg']['f1-score']:.3f}")
        
        print("\n5. RECOMENDACIONES:")
        best_model = max(self.results.keys(), key=lambda k: self.results[k]['accuracy'])
        print(f"   - Modelo recomendado: {best_model}")
        print(f"   - Consideraciones adicionales:")
        print(f"     * Interpretabilidad vs. Rendimiento")
        print(f"     * Tiempo de entrenamiento")
        print(f"     * Capacidad de generalizaci√≥n")
    
    def hyperparameter_tuning(self, model_name='Random Forest'):
        """Realiza ajuste de hiperpar√°metros para el modelo especificado"""
        print(f"\nüîß AJUSTE DE HIPERPAR√ÅMETROS - {model_name}")
        print("="*50)
        
        if model_name == 'Random Forest':
            param_grid = {
                'n_estimators': [50, 100, 200],
                'max_depth': [None, 10, 20],
                'min_samples_split': [2, 5, 10],
                'min_samples_leaf': [1, 2, 4]
            }
            model = RandomForestClassifier(random_state=42)
            
        elif model_name == 'SVM':
            param_grid = {
                'C': [0.1, 1, 10],
                'gamma': ['scale', 'auto', 0.1, 1],
                'kernel': ['rbf', 'poly']
            }
            model = SVC(random_state=42, probability=True)
            
        else:
            print("Modelo no soportado para tuning en esta demostraci√≥n")
            return
        
        # Grid Search
        X_use = self.X_train_scaled if model_name == 'SVM' else self.X_train
        grid_search = GridSearchCV(model, param_grid, cv=3, scoring='accuracy', n_jobs=-1)
        grid_search.fit(X_use, self.y_train)
        
        print(f"Mejores par√°metros: {grid_search.best_params_}")
        print(f"Mejor score CV: {grid_search.best_score_:.4f}")
        
        # Evaluar modelo optimizado
        X_test_use = self.X_test_scaled if model_name == 'SVM' else self.X_test
        best_model = grid_search.best_estimator_
        y_pred = best_model.predict(X_test_use)
        accuracy = accuracy_score(self.y_test, y_pred)
        
        print(f"Precisi√≥n en conjunto de prueba: {accuracy:.4f}")
        print(f"Mejora respecto al modelo base: {accuracy - self.results[model_name]['accuracy']:.4f}")
    
    def run_complete_analysis(self):
        """Ejecuta el an√°lisis completo"""
        print("üöÄ INICIANDO AN√ÅLISIS COMPLETO DE ALGORITMOS ML")
        print("="*60)
        
        # 1. Descargar y cargar datos
        self.download_and_load_data()
        
        # 2. An√°lisis exploratorio
        self.exploratory_data_analysis()
        
        # 3. Preparar datos
        self.prepare_data()
        
        # 4. Inicializar modelos
        self.initialize_models()
        
        # 5. Entrenar y evaluar
        self.train_and_evaluate_models()
        
        # 6. Crear visualizaciones
        self.create_comparison_visualizations()
        
        # 7. Tabla de resultados
        results_table = self.create_results_table()
        
        # 8. Reporte detallado
        self.generate_detailed_report()
        
        # 9. Ajuste de hiperpar√°metros del mejor modelo
        best_model = max(self.results.keys(), key=lambda k: self.results[k]['accuracy'])
        if best_model in ['Random Forest', 'SVM']:
            self.hyperparameter_tuning(best_model)
        
        print("\nüéâ AN√ÅLISIS COMPLETADO")
        print("="*30)
        print("Entregables generados:")
        print("‚úì An√°lisis exploratorio de datos")
        print("‚úì Comparaci√≥n de 5 algoritmos ML")
        print("‚úì Visualizaciones comprehensivas")
        print("‚úì Tabla comparativa de m√©tricas")
        print("‚úì Reporte detallado")
        print("‚úì Ajuste de hiperpar√°metros")
        
        return results_table

# Ejecutar v√≠a CLI
if __name__ == "__main__":
    parser = build_arg_parser()
    args = parser.parse_args()

    analyzer = CoffeeShopMLAnalysis()
    # Resoluci√≥n de datos con fallback
    analyzer.resolve_and_load_data(
        path=args.path,
        source=args.source,
        kaggle_dataset=args.kaggle_dataset,
        filename_hint=args.filename_hint,
        sep=args.sep,
        decimal=args.decimal,
        id_column=args.id_column,
        demo=args.demo,
    )
    print(f"Fuente de datos final: {analyzer.resolved_source}")
    print(f"Ruta utilizada: {analyzer.resolved_data_path}")

    # Ejecutar seg√∫n target
    analyzer.run_via_target(target=args.target)

    # Guardado de resultados si existen
    if analyzer.results:
        try:
            results_table = analyzer.create_results_table()
            results_table.to_csv('resultados_algoritmos_ml.csv', index=False)
            print("\nüìä Tabla de resultados guardada como 'resultados_algoritmos_ml.csv'")
        except Exception:
            pass