"""
M√≥dulo para comparaci√≥n y evaluaci√≥n de modelos de Machine Learning
Universidad Pedag√≥gica y Tecnol√≥gica de Colombia
Inteligencia Computacional
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from ..utils.constants import FILES
from ..utils.helpers import save_artifact
from ..utils.helpers import setup_matplotlib
import warnings
warnings.filterwarnings('ignore')

# Configurar matplotlib
setup_matplotlib()

class ModelComparator:
    """
    Clase para comparar y evaluar m√∫ltiples modelos de Machine Learning
    """
    
    def __init__(self):
        self.models = {}
        self.results = {}
        self.comparison_df = None
        
    def add_model(self, model, model_name):
        """
        A√±ade un modelo a la comparaci√≥n
        
        Args:
            model: Instancia del modelo entrenado
            model_name: Nombre del modelo
        """
        self.models[model_name] = model
        print(f"‚úÖ Modelo '{model_name}' a√±adido a la comparaci√≥n")
    
    def evaluate_all_models(self, X_test, y_test):
        """
        Eval√∫a todos los modelos a√±adidos
        
        Args:
            X_test: Datos de prueba
            y_test: Etiquetas de prueba
        """
        if not self.models:
            print("‚ùå No hay modelos para evaluar")
            return
        
        print("\n" + "="*80)
        print("üìä EVALUACI√ìN DE MODELOS")
        print("="*80)
        
        results_list = []
        
        for model_name, model in self.models.items():
            print(f"\nüîÑ Evaluando {model_name}...")
            
            # Realizar predicciones
            y_pred = model.predict(X_test)
            
            # Calcular m√©tricas
            mse = mean_squared_error(y_test, y_pred)
            rmse = np.sqrt(mse)
            mae = mean_absolute_error(y_test, y_pred)
            r2 = r2_score(y_test, y_pred)
            
            # Calcular m√©tricas adicionales
            mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100  # MAPE en %
            
            # Almacenar resultados
            result = {
                'Modelo': model_name,
                'MSE': round(mse, 4),
                'RMSE': round(rmse, 4),
                'MAE': round(mae, 4),
                'R¬≤': round(r2, 4),
                'MAPE (%)': round(mape, 2),
                'Predicciones': y_pred
            }
            
            self.results[model_name] = result
            results_list.append(result)
            
            print(f"   ‚Ä¢ MSE: {mse:,.2f}")
            print(f"   ‚Ä¢ RMSE: {rmse:,.2f}")
            print(f"   ‚Ä¢ MAE: {mae:,.2f}")
            print(f"   ‚Ä¢ R¬≤: {r2:.4f}")
            print(f"   ‚Ä¢ MAPE: {mape:.2f}%")
        
        # Crear DataFrame de comparaci√≥n
        self.comparison_df = pd.DataFrame(results_list)
        self.comparison_df = self.comparison_df.drop('Predicciones', axis=1)  # Remover predicciones del DF
        
        print(f"\n‚úÖ Evaluaci√≥n completada para {len(self.models)} modelos")
    
    def create_comparison_table(self):
        """
        Crea y muestra una tabla de comparaci√≥n
        """
        if self.comparison_df is None:
            print("‚ùå Primero debe evaluar los modelos")
            return
        
        print("\n" + "="*100)
        print("üìã TABLA DE COMPARACI√ìN DE MODELOS")
        print("="*100)
        
        # Mostrar tabla formateada
        pd.set_option('display.max_columns', None)
        pd.set_option('display.width', None)
        pd.set_option('display.max_colwidth', None)
        
        print(self.comparison_df.to_string(index=False))
        
        # Identificar el mejor modelo para cada m√©trica
        print(f"\nüèÜ MEJORES MODELOS POR M√âTRICA:")
        print(f"   ‚Ä¢ Mejor MSE (menor): {self.comparison_df.loc[self.comparison_df['MSE'].idxmin(), 'Modelo']}")
        print(f"   ‚Ä¢ Mejor RMSE (menor): {self.comparison_df.loc[self.comparison_df['RMSE'].idxmin(), 'Modelo']}")
        print(f"   ‚Ä¢ Mejor MAE (menor): {self.comparison_df.loc[self.comparison_df['MAE'].idxmin(), 'Modelo']}")
        print(f"   ‚Ä¢ Mejor R¬≤ (mayor): {self.comparison_df.loc[self.comparison_df['R¬≤'].idxmax(), 'Modelo']}")
        print(f"   ‚Ä¢ Mejor MAPE (menor): {self.comparison_df.loc[self.comparison_df['MAPE (%)'].idxmin(), 'Modelo']}")
    
    def plot_comparison_metrics(self):
        """
        Crea gr√°ficos de comparaci√≥n de m√©tricas
        """
        if self.comparison_df is None:
            print("‚ùå Primero debe evaluar los modelos")
            return
        
        # Configurar subplots
        fig, axes = plt.subplots(2, 3, figsize=(20, 12))
        fig.suptitle('üìä Comparaci√≥n de M√©tricas de Modelos', fontsize=16, fontweight='bold')
        
        # Colores para cada modelo
        colors = plt.cm.Set3(np.linspace(0, 1, len(self.comparison_df)))
        
        # 1. MSE
        axes[0, 0].bar(self.comparison_df['Modelo'], self.comparison_df['MSE'], color=colors)
        axes[0, 0].set_title('Error Cuadr√°tico Medio (MSE)', fontweight='bold')
        axes[0, 0].set_ylabel('MSE')
        axes[0, 0].tick_params(axis='x', rotation=45)
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. RMSE
        axes[0, 1].bar(self.comparison_df['Modelo'], self.comparison_df['RMSE'], color=colors)
        axes[0, 1].set_title('Ra√≠z del Error Cuadr√°tico Medio (RMSE)', fontweight='bold')
        axes[0, 1].set_ylabel('RMSE')
        axes[0, 1].tick_params(axis='x', rotation=45)
        axes[0, 1].grid(True, alpha=0.3)
        
        # 3. MAE
        axes[0, 2].bar(self.comparison_df['Modelo'], self.comparison_df['MAE'], color=colors)
        axes[0, 2].set_title('Error Absoluto Medio (MAE)', fontweight='bold')
        axes[0, 2].set_ylabel('MAE')
        axes[0, 2].tick_params(axis='x', rotation=45)
        axes[0, 2].grid(True, alpha=0.3)
        
        # 4. R¬≤
        axes[1, 0].bar(self.comparison_df['Modelo'], self.comparison_df['R¬≤'], color=colors)
        axes[1, 0].set_title('Coeficiente de Determinaci√≥n (R¬≤)', fontweight='bold')
        axes[1, 0].set_ylabel('R¬≤')
        axes[1, 0].tick_params(axis='x', rotation=45)
        axes[1, 0].grid(True, alpha=0.3)
        
        # 5. MAPE
        axes[1, 1].bar(self.comparison_df['Modelo'], self.comparison_df['MAPE (%)'], color=colors)
        axes[1, 1].set_title('Error Porcentual Absoluto Medio (MAPE)', fontweight='bold')
        axes[1, 1].set_ylabel('MAPE (%)')
        axes[1, 1].tick_params(axis='x', rotation=45)
        axes[1, 1].grid(True, alpha=0.3)
        
        # 6. Comparaci√≥n m√∫ltiple (normalizada)
        metrics_to_plot = ['MSE', 'RMSE', 'MAE', 'MAPE (%)']
        normalized_data = self.comparison_df[metrics_to_plot].copy()
        
        # Normalizar datos (0-1)
        for col in metrics_to_plot:
            normalized_data[col] = (normalized_data[col] - normalized_data[col].min()) / (normalized_data[col].max() - normalized_data[col].min())
        
        x = np.arange(len(self.comparison_df['Modelo']))
        width = 0.2
        
        for i, metric in enumerate(metrics_to_plot):
            axes[1, 2].bar(x + i*width, normalized_data[metric], width, 
                          label=metric, alpha=0.8)
        
        axes[1, 2].set_title('Comparaci√≥n Normalizada de M√©tricas', fontweight='bold')
        axes[1, 2].set_ylabel('Valor Normalizado (0-1)')
        axes[1, 2].set_xticks(x + width * 1.5)
        axes[1, 2].set_xticklabels(self.comparison_df['Modelo'], rotation=45)
        axes[1, 2].legend()
        axes[1, 2].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
    
    def plot_predictions_vs_actual(self, y_test):
        """
        Crea gr√°ficos de predicciones vs valores reales
        """
        if not self.results:
            print("‚ùå Primero debe evaluar los modelos")
            return
        
        n_models = len(self.results)
        cols = 2
        rows = (n_models + 1) // 2
        
        fig, axes = plt.subplots(rows, cols, figsize=(15, 5*rows))
        fig.suptitle('üéØ Predicciones vs Valores Reales', fontsize=16, fontweight='bold')
        
        if n_models == 1:
            axes = [axes]
        elif rows == 1:
            axes = axes.reshape(1, -1)
        
        for i, (model_name, result) in enumerate(self.results.items()):
            row = i // cols
            col = i % cols
            
            if rows == 1:
                ax = axes[col]
            else:
                ax = axes[row, col]
            
            y_pred = result['Predicciones']
            
            # Scatter plot
            ax.scatter(y_test, y_pred, alpha=0.6, s=50)
            
            # L√≠nea perfecta (y=x)
            min_val = min(y_test.min(), y_pred.min())
            max_val = max(y_test.max(), y_pred.max())
            ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Predicci√≥n Perfecta')
            
            # Configuraci√≥n del gr√°fico
            ax.set_xlabel('Valores Reales', fontweight='bold')
            ax.set_ylabel('Predicciones', fontweight='bold')
            ax.set_title(f'{model_name}\nR¬≤ = {result["R¬≤"]:.4f}', fontweight='bold')
            ax.grid(True, alpha=0.3)
            ax.legend()
            
            # A√±adir estad√≠sticas
            ax.text(0.05, 0.95, f'RMSE: {result["RMSE"]:.2f}\nMAE: {result["MAE"]:.2f}', 
                   transform=ax.transAxes, verticalalignment='top',
                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
        
        # Ocultar subplots vac√≠os
        for i in range(n_models, rows * cols):
            row = i // cols
            col = i % cols
            if rows == 1:
                axes[col].set_visible(False)
            else:
                axes[row, col].set_visible(False)
        
        plt.tight_layout()
        plt.show()
    
    def plot_feature_importance(self, feature_names):
        """
        Crea gr√°ficos de importancia de caracter√≠sticas
        """
        if not self.models:
            print("‚ùå No hay modelos para analizar")
            return
        
        # Filtrar modelos que tienen importancia de caracter√≠sticas
        models_with_importance = {}
        for name, model in self.models.items():
            if hasattr(model, 'get_feature_importance') and model.get_feature_importance() is not None:
                models_with_importance[name] = model
        
        if not models_with_importance:
            print("‚ÑπÔ∏è  Ning√∫n modelo tiene informaci√≥n de importancia de caracter√≠sticas")
            return
        
        n_models = len(models_with_importance)
        cols = 2
        rows = (n_models + 1) // 2
        
        fig, axes = plt.subplots(rows, cols, figsize=(15, 5*rows))
        fig.suptitle('üîç Importancia de Caracter√≠sticas por Modelo', fontsize=16, fontweight='bold')
        
        if n_models == 1:
            axes = [axes]
        elif rows == 1:
            axes = axes.reshape(1, -1)
        
        for i, (model_name, model) in enumerate(models_with_importance.items()):
            row = i // cols
            col = i % cols
            
            if rows == 1:
                ax = axes[col]
            else:
                ax = axes[row, col]
            
            importance = model.get_feature_importance()
            
            # Crear gr√°fico de barras horizontal
            y_pos = np.arange(len(feature_names))
            ax.barh(y_pos, importance, alpha=0.7)
            ax.set_yticks(y_pos)
            ax.set_yticklabels(feature_names)
            ax.set_xlabel('Importancia', fontweight='bold')
            ax.set_title(f'{model_name}', fontweight='bold')
            ax.grid(True, alpha=0.3)
        
        # Ocultar subplots vac√≠os
        for i in range(n_models, rows * cols):
            row = i // cols
            col = i % cols
            if rows == 1:
                axes[col].set_visible(False)
            else:
                axes[row, col].set_visible(False)
        
        plt.tight_layout()
        plt.show()
    
    def get_best_model(self, metric='R¬≤'):
        """
        Retorna el mejor modelo seg√∫n la m√©trica especificada
        
        Args:
            metric: M√©trica para evaluar ('MSE', 'RMSE', 'MAE', 'R¬≤', 'MAPE (%)')
            
        Returns:
            tuple: (nombre_del_modelo, valor_de_la_m√©trica)
        """
        if self.comparison_df is None:
            print("‚ùå Primero debe evaluar los modelos")
            return None
        
        if metric in ['MSE', 'RMSE', 'MAE', 'MAPE (%)']:
            # Para estas m√©tricas, menor es mejor
            best_idx = self.comparison_df[metric].idxmin()
        else:
            # Para R¬≤, mayor es mejor
            best_idx = self.comparison_df[metric].idxmax()
        
        best_model = self.comparison_df.loc[best_idx, 'Modelo']
        best_value = self.comparison_df.loc[best_idx, metric]
        
        return best_model, best_value
    
    def save_results(self, filename=None):
        """
        Guarda los resultados en un archivo CSV
        
        Args:
            filename: Nombre del archivo
        """
        if self.comparison_df is None:
            print("‚ùå No hay resultados para guardar")
            return
        
        if filename is None:
            filename = FILES['results_csv']
        
        self.comparison_df.to_csv(filename, index=False)
        print(f"‚úÖ Resultados guardados en {filename}")
        # Guardar un snapshot de resultados tambi√©n como artefacto pkl para trazabilidad
        try:
            save_artifact(self.comparison_df, filename.replace('.csv', '.pkl'), metadata={'type': 'comparison_df'})
            print(f"[RESULTS] Snapshot saved: {filename.replace('.csv', '.pkl')}")
        except Exception:
            pass
    
    def generate_report(self):
        """
        Genera un reporte completo de la comparaci√≥n
        """
        if self.comparison_df is None:
            print("‚ùå Primero debe evaluar los modelos")
            return
        
        print("\n" + "="*100)
        print("üìã REPORTE COMPLETO DE COMPARACI√ìN DE MODELOS")
        print("="*100)
        
        # Resumen general
        print(f"\nüìä RESUMEN GENERAL:")
        print(f"   ‚Ä¢ Total de modelos evaluados: {len(self.comparison_df)}")
        print(f"   ‚Ä¢ Mejor modelo general (R¬≤): {self.get_best_model('R¬≤')[0]}")
        print(f"   ‚Ä¢ R¬≤ promedio: {self.comparison_df['R¬≤'].mean():.4f}")
        print(f"   ‚Ä¢ RMSE promedio: {self.comparison_df['RMSE'].mean():.2f}")
        
        # Ranking de modelos
        print(f"\nüèÜ RANKING DE MODELOS (por R¬≤):")
        ranked_models = self.comparison_df.sort_values('R¬≤', ascending=False)
        for i, (_, row) in enumerate(ranked_models.iterrows(), 1):
            print(f"   {i}. {row['Modelo']}: R¬≤ = {row['R¬≤']:.4f}")
        
        # An√°lisis de m√©tricas
        print(f"\nüìà AN√ÅLISIS DE M√âTRICAS:")
        print(f"   ‚Ä¢ Rango de R¬≤: {self.comparison_df['R¬≤'].min():.4f} - {self.comparison_df['R¬≤'].max():.4f}")
        print(f"   ‚Ä¢ Rango de RMSE: {self.comparison_df['RMSE'].min():.2f} - {self.comparison_df['RMSE'].max():.2f}")
        print(f"   ‚Ä¢ Rango de MAE: {self.comparison_df['MAE'].min():.2f} - {self.comparison_df['MAE'].max():.2f}")
        
        # Recomendaciones
        print(f"\nüí° RECOMENDACIONES:")
        best_r2_model = self.get_best_model('R¬≤')[0]
        best_rmse_model = self.get_best_model('RMSE')[0]
        
        if best_r2_model == best_rmse_model:
            print(f"   ‚Ä¢ El modelo '{best_r2_model}' es el mejor en general")
        else:
            print(f"   ‚Ä¢ Para m√°xima precisi√≥n: usar '{best_r2_model}' (mejor R¬≤)")
            print(f"   ‚Ä¢ Para menor error: usar '{best_rmse_model}' (mejor RMSE)")
        
        print(f"   ‚Ä¢ Considerar el balance entre precisi√≥n y interpretabilidad")
        print(f"   ‚Ä¢ Validar con datos adicionales antes de implementar en producci√≥n")
